File Name: nmt-master/nmt/attention_model.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 15
"""Attention-based sequence-to-sequence model with dynamic RNN support."""

Line number: 29
"""Sequence-to-sequence dynamic model with attention.

This class implements a multi-layer recurrent neural network as encoder,
and an attention-based decoder. This is the same as the model described in
(Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.
This class also allows to use GRU cells in addition to LSTM cells with
support for dropout.
"""

Line number: 49
# Set attention_mechanism_fn

Line number: 79
"""Build a RNN cell with attention mechanism that can be used by decoder."""

Line number: 80
# No Attention

Line number: 95
# Ensure memory is batch-major

Line number: 110
# Attention

Line number: 125
# Only generate alignment in greedy INFER mode.

Line number: 136
# TODO(thangluong): do we need num_layers, num_gpus?

Line number: 157
"""Create attention mechanism based on the attention_option."""

Line number: 158
del mode  # unused

Line number: 160
# Mechanism

Line number: 186
"""create attention image and attention summary."""

Line number: 188
# Reshape to (batch, src_seq_len, tgt_seq_len,1)

Line number: 191
# Scale to range [0, 255]


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/gnmt_model.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""GNMT attention sequence-to-sequence model with dynamic RNN support."""

Line number: 32
"""Sequence-to-sequence dynamic model with GNMT attention architecture.
"""

Line number: 58
"""Build a GNMT encoder."""

Line number: 65
# Build GNMT encoder.

Line number: 83
# Execute _build_bidirectional_rnn from Model class

Line number: 90
num_bi_residual_layers=0,  # no residual connection

Line number: 93
# Build unidirectional layers

Line number: 101
# Pass all encoder states to the decoder

Line number: 102
#   except the first bi-directional layer

Line number: 110
"""Build encoder layers all at once."""

Line number: 129
# Use the top layer for now

Line number: 136
"""Run each of the encoder layer separately, not used in general seq2seq."""

Line number: 172
"""Build a RNN cell with GNMT attention architecture."""

Line number: 173
# Standard attention

Line number: 178
# GNMT attention

Line number: 203
cell_list = model_helper._cell_list(  # pylint: disable=protected-access

Line number: 216
# Only wrap the bottom layer with the attention mechanism.

Line number: 219
# Only generate alignment in greedy INFER mode.

Line number: 225
attention_layer_size=None,  # don't use attention layer.

Line number: 262
"""A MultiCell with GNMT attention style."""

Line number: 265
"""Creates a GNMTAttentionMultiCell.

Args:
attention_cell: An instance of AttentionWrapper.
cells: A list of RNNCell wrapped with AttentionInputWrapper.
use_new_attention: Whether to use the attention generated from current
step bottom layer's output. Default is False.
"""

Line number: 278
"""Run the cell with bottom layer's attention copied to all upper layers."""

Line number: 311
"""Residual function that handles different inputs and outputs inner dims.

Args:
inputs: cell inputs, this is actual inputs concatenated with the attention
vector.
outputs: cell outputs

Returns:
outputs + actual inputs
"""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/inference.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""To perform inference on test set given a trained model."""

Line number: 40
"""Decoding only a specific set of sentences."""

Line number: 46
trans_f.write("")  # Write empty string to ensure file is created.

Line number: 50
# get text translation

Line number: 58
if infer_summary is not None:  # Attention models

Line number: 72
"""Load inference data."""

Line number: 84
"""Get the right model class depending on configuration."""

Line number: 99
"""Start session and load model."""

Line number: 115
"""Perform translation."""

Line number: 150
"""Inference with a single worker."""

Line number: 153
# Read data

Line number: 163
# Decode

Line number: 197
"""Inference using multiple workers."""

Line number: 204
# Read data

Line number: 207
# Split data to multiple workers

Line number: 220
# Decode

Line number: 235
# Change file name to indicate the file writing is completed.

Line number: 238
# Job 0 is responsible for the clean up.

Line number: 241
# Now write all translations


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/inference_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Tests for model inference."""

Line number: 38
# Prepare

Line number: 47
# Create check point

Line number: 115
# There are 5 examples, make batch_size=3 makes job0 has 3 examples, job1

Line number: 116
# has 2 examples, and job2 has 0 example. This helps testing some edge

Line number: 117
# cases.

Line number: 129
# Note: Need to start job 0 at the end; otherwise, it will block the testing

Line number: 130
# thread.

Line number: 161
# TODO(rzhao): Make infer indices support batch_size > 1.


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/__init__.py


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/model_helper.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Utility functions for building models."""

Line number: 39
# If a vocab size is greater than this value, put the embedding on cpu instead

Line number: 44
"""Create an initializer. init_weight is only for uniform."""

Line number: 60
"""Return a device string for multi-GPU setup."""

Line number: 82
"""Create train graph, model, and iterator."""

Line number: 115
# Note: One can set model_device_fn to

Line number: 116
# `tf.train.replica_device_setter(ps_tasks)` for distributed training.

Line number: 144
"""Create train graph, model, src/tgt file holders, and iterator."""

Line number: 197
"""Create inference model."""

Line number: 238
"""Decide on which device to place an embed matrix given its vocab size."""

Line number: 248
"""Load pretrain embeding from embed_file, and return an embedding matrix.

Args:
embed_file: Path to a Glove formated embedding txt file.
num_trainable_tokens: Make the first n tokens in the vocab file as trainable
variables. Default is 3, which is "<unk>", "<s>" and "</s>".
"""

Line number: 280
"""Create a new or load an existing embedding matrix."""

Line number: 304
"""Create embedding matrix for both encoder and decoder.

Args:
share_vocab: A boolean. Whether to share embedding matrix for both
encoder and decoder.
src_vocab_size: An integer. The source vocab size.
tgt_vocab_size: An integer. The target vocab size.
src_embed_size: An integer. The embedding dimension for the encoder's
embedding.
tgt_embed_size: An integer. The embedding dimension for the decoder's
embedding.
dtype: dtype of the embedding matrix. Default to float32.
num_enc_partitions: number of partitions used for the encoder's embedding
vars.
num_dec_partitions: number of partitions used for the decoder's embedding
vars.
scope: VariableScope for the created subgraph. Default to "embedding".

Returns:
embedding_encoder: Encoder's embedding matrix.
embedding_decoder: Decoder's embedding matrix.

Raises:
ValueError: if use share_vocab but source and target have different vocab
size.
"""

Line number: 333
# Note: num_partitions > 1 is required for distributed training due to

Line number: 334
# embedding_lookup tries to colocate single partition-ed embedding variable

Line number: 335
# with lookup ops. This may cause embedding variables being placed on worker

Line number: 336
# jobs.

Line number: 342
# Note: num_partitions > 1 is required for distributed training due to

Line number: 343
# embedding_lookup tries to colocate single partition-ed embedding variable

Line number: 344
# with lookup ops. This may cause embedding variables being placed on worker

Line number: 345
# jobs.

Line number: 360
# Share embedding

Line number: 393
"""Create an instance of a single RNN cell."""

Line number: 394
# dropout (= 1 - keep_prob) is set to 0 during eval and infer

Line number: 397
# Cell Type

Line number: 419
# Dropout (= 1 - keep_prob)

Line number: 426
# Residual

Line number: 432
# Device Wrapper

Line number: 444
"""Create a list of RNN cells."""

Line number: 448
# Multi-GPU

Line number: 471
"""Create multi-layer RNN cell.

Args:
unit_type: string representing the unit type, i.e. "lstm".
num_units: the depth of each unit.
num_layers: number of cells.
num_residual_layers: Number of residual layers from top to bottom. For
example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN
cells in the returned list will be wrapped with `ResidualWrapper`.
forget_bias: the initial forget bias of the RNNCell(s).
dropout: floating point value between 0.0 and 1.0:
the probability of dropout.  this is ignored if `mode != TRAIN`.
mode: either tf.contrib.learn.TRAIN/EVAL/INFER
num_gpus: The number of gpus to use when performing round-robin
placement of layers.
base_gpu: The gpu device id to use for the first RNN cell in the
returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`
as its device id.
single_cell_fn: allow for adding customized cell.
When not specified, we default to model_helper._single_cell
Returns:
An `RNNCell` instance.
"""

Line number: 505
if len(cell_list) == 1:  # Single layer.

Line number: 507
else:  # Multi layers

Line number: 512
"""Clipping gradients of a model."""

Line number: 523
"""Print a list of variables in a checkpoint together with their shapes."""

Line number: 532
"""Load model from a checkpoint."""

Line number: 550
"""Average the last N checkpoints in the model_dir."""

Line number: 556
# Checkpoints are ordered from oldest to newest.

Line number: 592
# Build a graph with same variables in the checkpoints, and save the averaged

Line number: 593
# variables into the avg_model_dir.

Line number: 612
# Use the built saver to save the averaged checkpoint. Only keep 1

Line number: 613
# checkpoint and the best checkpoint will be moved to avg_best_metric_dir.

Line number: 622
"""Create translation model and initialize or load parameters in session."""

Line number: 638
"""Compute perplexity of the output of the model.

Args:
model: model for compute perplexity.
sess: tensorflow session to use.
name: name of the batch.

Returns:
The perplexity of the eval outputs.
"""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/model.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Basic sequence-to-sequence model with dynamic RNN support."""

Line number: 41
"""To allow for flexibily in returing different outputs."""

Line number: 47
"""To allow for flexibily in returing different outputs."""

Line number: 54
"""To allow for flexibily in returing different outputs."""

Line number: 59
"""Sequence-to-sequence base class.
"""

Line number: 71
"""Create the model.

Args:
hparams: Hyperparameter configurations.
mode: TRAIN | EVAL | INFER
iterator: Dataset Iterator that feeds data.
source_vocab_table: Lookup table mapping source words to ids.
target_vocab_table: Lookup table mapping target words to ids.
reverse_target_vocab_table: Lookup table mapping ids to target words. Only
required in INFER mode. Defaults to None.
scope: scope of the model.
extra_args: model_helper.ExtraArgs, for passing customizable functions.

"""

Line number: 85
# Set params

Line number: 90
# Not used in general seq2seq models; when True, ignore decoder & training

Line number: 94
# Train graph

Line number: 99
# Saver

Line number: 111
"""Set various params for self and initialize."""

Line number: 130
# extra_args: to make it flexible for adding external customizable code

Line number: 135
# Set num units

Line number: 138
# Set num layers

Line number: 144
# Set num residual layers

Line number: 145
if hasattr(hparams, "num_residual_layers"):  # compatible common_test_utils

Line number: 152
# Batch size

Line number: 155
# Global step

Line number: 158
# Initializer

Line number: 164
# Embeddings

Line number: 172
"""Set up training and inference."""

Line number: 186
## Count the number of predicted words for compute ppl.

Line number: 192
# Gradients and SGD update operation for training the model.

Line number: 193
# Arrange for the embedding vars to appear at the beginning.

Line number: 196
# warm-up

Line number: 198
# decay

Line number: 201
# Optimizer

Line number: 209
# Gradients

Line number: 223
# Summary

Line number: 228
# Print trainable variables

Line number: 236
"""Get learning rate warmup."""

Line number: 242
# Apply inverse decay if global steps less than warmup steps.

Line number: 243
# Inspired by https://arxiv.org/pdf/1706.03762.pdf (Section 5.3)

Line number: 244
# When step < warmup_steps,

Line number: 245
#   learing_rate *= warmup_factor ** (warmup_steps - step)

Line number: 247
# 0.01^(1/warmup_steps): we start with a lr, 100 times smaller

Line number: 261
"""Return decay info based on decay_scheme."""

Line number: 275
elif not hparams.decay_scheme:  # no decay

Line number: 284
"""Get learning rate decay."""

Line number: 302
"""Init embeddings."""

Line number: 320
"""Get train summary."""

Line number: 328
"""Execute train graph."""

Line number: 341
"""Execute eval graph."""

Line number: 349
"""Subclass must implement this method.

Creates a sequence-to-sequence model with dynamic RNN decoder API.
Args:
hparams: Hyperparameter configurations.
scope: VariableScope for the created subgraph; default "dynamic_seq2seq".

Returns:
A tuple of the form (logits, loss_tuple, final_context_state, sample_id),
where:
logits: float32 Tensor [batch_size x num_decoder_symbols].
loss: loss = the total loss / batch_size.
final_context_state: the final state of decoder RNN.
sample_id: sampling indices.

Raises:
ValueError: if encoder_type differs from mono and bi, or
attention_option is not (luong | scaled_luong |
bahdanau | normed_bahdanau).
"""

Line number: 371
# Projection

Line number: 379
# Encoder

Line number: 380
if hparams.language_model:  # no encoder for language modeling

Line number: 387
# Skip decoder if extracting only encoder layers

Line number: 391
## Decoder

Line number: 395
## Loss

Line number: 407
"""Subclass must implement this.

Build and run an RNN encoder.

Args:
hparams: Hyperparameters configurations.

Returns:
A tuple of encoder_outputs and encoder_state.
"""

Line number: 421
"""Build a multi-layer RNN cell that can be used by encoder."""

Line number: 436
"""Maximum decoding steps at inference time."""

Line number: 441
# TODO(thangluong): add decoding_length_factor flag

Line number: 449
"""Build and run a RNN decoder with a final projection layer.

Args:
encoder_outputs: The outputs of encoder for every time step.
encoder_state: The final state of the encoder.
hparams: The Hyperparameters configurations.

Returns:
A tuple of final logits and final decoder state:
logits: size [time, batch_size, vocab_size] when time_major=True.
"""

Line number: 466
# maximum_iteration: The maximum decoding steps.

Line number: 470
## Decoder.

Line number: 476
# Optional ops depends on which mode we are in and which loss function we

Line number: 477
# are using.

Line number: 481
## Train or eval

Line number: 483
# decoder_emp_inp: [max_time, batch_size, num_units]

Line number: 490
# Helper

Line number: 495
# Decoder

Line number: 501
# Dynamic decoding

Line number: 511
# Note: this is required when using sampled_softmax_loss.

Line number: 514
# Note: there's a subtle difference here between train and inference.

Line number: 515
# We could have set output_layer when create my_decoder

Line number: 516
#   and shared more code between train and inference.

Line number: 517
# We chose to apply the output_layer to all timesteps for speed:

Line number: 518
#   10% improvements for small models & 20% for larger ones.

Line number: 519
# If memory is a concern, we should apply output_layer per timestep.

Line number: 523
# Colocate output layer with the last RNN cell if there is no extra GPU

Line number: 524
# available. Otherwise, put last layer on a separate GPU.

Line number: 529
logits = tf.no_op()  # unused when using sampled softmax loss.

Line number: 531
## Inference

Line number: 558
# Helper

Line number: 578
output_layer=self.output_layer  # applied per timestep

Line number: 581
# Dynamic decoding

Line number: 604
"""Subclass must implement this.

Args:
hparams: Hyperparameters configurations.
encoder_outputs: The outputs of encoder for every time step.
encoder_state: The final state of the encoder.
source_sequence_length: sequence length of encoder_outputs.

Returns:
A tuple of a multi-layer RNN cell used by decoder and the intial state of
the decoder RNN.
"""

Line number: 620
"""Compute softmax loss or sampled softmax loss."""

Line number: 652
"""Compute optimization loss."""

Line number: 683
"""Decode a batch.

Args:
sess: tensorflow session to use.

Returns:
A tuple consiting of outputs, infer_summary.
outputs: of size [batch_size, time]
"""

Line number: 696
# make sure outputs is of shape [batch_size, time] or [beam_width,

Line number: 697
# batch_size, time] when using beam search.

Line number: 701
# beam search output in [batch_size, time, beam_width] shape.

Line number: 706
"""Stack encoder states and return tensor [batch, length, layer, size]."""

Line number: 714
# transform from [length, batch, ...] -> [batch, length, ...]

Line number: 722
"""Sequence-to-sequence dynamic model.

This class implements a multi-layer recurrent neural network as encoder,
and a multi-layer recurrent neural network decoder.
"""

Line number: 728
"""Build an encoder from a sequence.

Args:
hparams: hyperparameters.
sequence: tensor with input sequence data.
sequence_length: tensor with length of the input sequence.

Returns:
encoder_outputs: RNN encoder outputs.
encoder_state: RNN encoder state.

Raises:
ValueError: if encoder_type is neither "uni" nor "bi".
"""

Line number: 754
# Encoder_outputs: [max_time, batch_size, num_units]

Line number: 786
# alternatively concat forward and backward states

Line number: 789
encoder_state.append(bi_encoder_state[0][layer_id])  # forward

Line number: 790
encoder_state.append(bi_encoder_state[1][layer_id])  # backward

Line number: 795
# Use the top layer for now

Line number: 801
"""Build encoder from source."""

Line number: 811
"""Create and call biddirectional RNN cells.

Args:
num_residual_layers: Number of residual layers from top to bottom. For
example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN
layers in each RNN cell will be wrapped with `ResidualWrapper`.
base_gpu: The gpu device id to use for the first forward RNN layer. The
i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its
device id. The `base_gpu` for backward RNN cell is `(base_gpu +
num_bi_layers)`.

Returns:
The concatenated bidirectional output and the bidirectional RNN cell"s
state.
"""

Line number: 826
# Construct forward and backward cells

Line number: 849
"""Build an RNN cell that can be used by decoder."""

Line number: 850
# We only make use of encoder_outputs in attention-based models

Line number: 873
# For beam search, we need to replicate encoder infos beam_width times


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/model_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 15
"""Tests for model.py."""

Line number: 360
## Testing 3 encoders:

Line number: 361
# uni: no attention, no residual, 1 layers

Line number: 362
# bi: no attention, with residual, 4 layers

Line number: 374
# pylint: disable=line-too-long

Line number: 384
# pylint: enable=line-too-long

Line number: 431
# pylint: disable=line-too-long

Line number: 453
# pylint: enable=line-too-long

Line number: 489
## Test attention mechanisms: luong, scaled_luong, bahdanau, normed_bahdanau

Line number: 501
# pylint: disable=line-too-long

Line number: 517
# pylint: enable=line-too-long

Line number: 529
# pylint: disable=line-too-long

Line number: 536
# pylint: enable=line-too-long

Line number: 569
# pylint: disable=line-too-long

Line number: 586
# pylint: enable=line-too-long

Line number: 598
# pylint: disable=line-too-long

Line number: 605
# pylint: enable=line-too-long

Line number: 643
# pylint: disable=line-too-long

Line number: 661
# pylint: enable=line-too-long

Line number: 673
# pylint: disable=line-too-long

Line number: 680
# pylint: enable=line-too-long

Line number: 713
# pylint: disable=line-too-long

Line number: 733
# pylint: enable=line-too-long

Line number: 746
# pylint: disable=line-too-long

Line number: 753
# pylint: enable=line-too-long

Line number: 780
## Test encoder vs. attention (all use residual):

Line number: 781
# uni encoder, standard attention

Line number: 792
# pylint: disable=line-too-long

Line number: 817
# pylint: enable=line-too-long

Line number: 860
# Test gnmt model.

Line number: 871
# pylint: disable=line-too-long

Line number: 897
# pylint: enable=line-too-long

Line number: 940
# Test beam search.


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/nmt.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""TensorFlow NMT model implementation."""

Line number: 24
# import matplotlib.image as mpimg

Line number: 46
"""Build ArgumentParser."""

Line number: 49
# network

Line number: 72
# attention mechanisms

Line number: 105
# optimizer

Line number: 135
# initializer

Line number: 142
# data

Line number: 156
# Vocab

Line number: 180
# Sequence lengths

Line number: 193
# Default settings works well (rarely need to change)

Line number: 215
# SPM

Line number: 222
# Experimental encoding feature.

Line number: 230
# Misc

Line number: 264
# Inference

Line number: 281
# Advanced inference arguments

Line number: 306
# Job info

Line number: 318
"""Create training hparams."""

Line number: 320
# Data

Line number: 330
# Networks

Line number: 341
# Attention mechanisms

Line number: 347
# Train

Line number: 361
# Data constraints

Line number: 367
# Inference

Line number: 372
# Advanced inference arguments

Line number: 380
# Vocab

Line number: 387
# Misc

Line number: 390
epoch_step=0,  # record where we were within an epoch.

Line number: 407
"""Add an argument to hparams; if exists, change the value if update==True."""

Line number: 416
"""Add new arguments to hparams."""

Line number: 417
# Sanity checks

Line number: 435
# Different number of encoder / decoder layers

Line number: 444
# Set residual layers

Line number: 454
# The first unidirectional layer (after the bi-directional layer) in

Line number: 455
# the GNMT encoder can't have residual connection due to the input is

Line number: 456
# the concatenation of fw_cell and bw_cell's outputs.

Line number: 459
# Compatible for GNMT models

Line number: 467
# Language modeling

Line number: 477
## Vocab

Line number: 478
# Get vocab file names first

Line number: 485
# Source vocab

Line number: 495
# Target vocab

Line number: 513
# Num embedding partitions

Line number: 518
# Pretrained Embeddings

Line number: 545
# Evaluation

Line number: 562
"""Make sure the loaded hparams is compatible with new changes."""

Line number: 566
# Set num encoder/decoder layers (for old checkpoints)

Line number: 573
# For compatible reason, if there are new fields in default_hparams,

Line number: 574
#   we add them to the current hparams

Line number: 581
# Update all hparams' keys if override_loaded_hparams=True

Line number: 585
# For inference

Line number: 599
"""Create hparams or load hparams from out_dir."""

Line number: 609
# Save HParams

Line number: 615
# Print HParams

Line number: 621
"""Run main."""

Line number: 622
# Job

Line number: 627
# GPU device

Line number: 631
# Random

Line number: 638
# Model output directory

Line number: 644
# Load hparams.

Line number: 646
if flags.ckpt:  # Try to load hparams from the same directory as ckpt

Line number: 654
if not loaded_hparams:  # Try to load from out_dir

Line number: 660
## Train / Decode

Line number: 662
# Inference output directory

Line number: 668
# Inference indices

Line number: 674
# Inference

Line number: 681
# Evaluation

Line number: 692
# Train


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/nmt_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 15
"""Tests for nmt.py, train.py and inference.py."""

Line number: 32
"""Update flags for basic training."""

Line number: 51
"""Test the training loop is functional with basic hparams."""

Line number: 65
"""Test the training loop is functional with basic hparams."""

Line number: 80
"""Test inference is function with basic hparams."""

Line number: 87
# Train one step so we have a checkpoint.

Line number: 93
# Update FLAGS for inference.


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/scripts/bleu.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Python implementation of BLEU and smooth-BLEU.

This module provides a Python implementation of BLEU and smooth-BLEU.
Smooth BLEU is computed following the method outlined in the paper:
Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic
evaluation metrics for machine translation. COLING 2004.
"""

Line number: 29
"""Extracts all n-grams upto a given maximum order from an input segment.

Args:
segment: text segment from which n-grams will be extracted.
max_order: maximum length in tokens of the n-grams returned by this
methods.

Returns:
The Counter containing all n-grams upto max_order in segment
with a count of how many times each n-gram occurred.
"""

Line number: 50
"""Computes BLEU score of translated segments against one or more references.

Args:
reference_corpus: list of lists of references for each translation. Each
reference should be tokenized into a list of tokens.
translation_corpus: list of translations to score. Each translation
should be tokenized into a list of tokens.
max_order: Maximum n-gram order to use when computing BLEU score.
smooth: Whether or not to apply Lin et al. 2004 smoothing.

Returns:
3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
precisions and brevity penalty.
"""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/scripts/__init__.py


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/scripts/rouge.py

Line number: 1
"""ROUGE metric implementation.

Copy from tf_seq2seq/seq2seq/metrics/rouge.py.
This is a modified and slightly extended verison of
https://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.
"""

Line number: 16
#pylint: disable=C0103

Line number: 20
"""Calcualtes n-grams.

Args:
n: which n-grams to calculate
text: An array of tokens

Returns:
A set of n-grams
"""

Line number: 38
"""Splits multiple sentences into words and flattens the result"""

Line number: 43
"""Calculates word n-grams for multiple sentences.
"""

Line number: 113
"""private recon calculation"""

Line number: 152
# Gets the overlapping ngrams between evaluated and reference

Line number: 156
# Handle edge case. This isn't mathematically correct, but it's good enough

Line number: 169
# return overlapping_count / reference_count

Line number: 301
# total number of words in reference sentences

Line number: 304
# total number of words in evaluated sentences

Line number: 315
"""Calculates average rouge scores for a list of hypotheses and
references"""

Line number: 318
# Filter out hyps that are of 0 length

Line number: 319
# hyps_and_refs = zip(hypotheses, references)

Line number: 320
# hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]

Line number: 321
# hypotheses, references = zip(*hyps_and_refs)

Line number: 323
# Calculate ROUGE-1 F1, precision, recall scores

Line number: 329
# Calculate ROUGE-2 F1, precision, recall scores

Line number: 335
# Calculate ROUGE-L F1, precision, recall scores


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/train.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 15
"""For training NMT models."""

Line number: 45
"""Sample decode a random sentence from src_data."""

Line number: 64
"""Compute internal evaluation (perplexity) for both dev / test.

Computes development and testing perplexities for given model.

Args:
eval_model: Evaluation model for which to compute perplexities.
eval_sess: Evaluation TensorFlow session.
model_dir: Directory from which to load evaluation model from.
hparams: Model hyper-parameters.
summary_writer: Summary writer for logging metrics to TensorBoard.
use_test_set: Computes testing perplexity if true; does not otherwise.
Note that the development perplexity is always computed regardless of
value of this parameter.
dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
development evaluation.
test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
testing evaluation.
Returns:
Pair containing development perplexity and testing perplexity, in this
order.
"""

Line number: 127
"""Compute external evaluation for both dev / test.

Computes development and testing external evaluation (e.g. bleu, rouge) for
given model.

Args:
infer_model: Inference model for which to compute perplexities.
infer_sess: Inference TensorFlow session.
model_dir: Directory from which to load inference model from.
hparams: Model hyper-parameters.
summary_writer: Summary writer for logging metrics to TensorBoard.
use_test_set: Computes testing external evaluation if true; does not
otherwise. Note that the development external evaluation is always
computed regardless of value of this parameter.
dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
development external evaluation.
test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
testing external evaluation.
Returns:
Triple containing development scores, testing scores and the TensorFlow
Variable for the global step number, in this order.
"""

Line number: 203
"""Creates an averaged checkpoint and run external eval with it."""

Line number: 206
# Convert VariableName:0 to VariableName.

Line number: 235
"""Compute internal evaluation (perplexity) for both dev / test.

Computes development and testing perplexities for given model.

Args:
model_dir: Directory from which to load models from.
infer_model: Inference model for which to compute perplexities.
infer_sess: Inference TensorFlow session.
eval_model: Evaluation model for which to compute perplexities.
eval_sess: Evaluation TensorFlow session.
hparams: Model hyper-parameters.
summary_writer: Summary writer for logging metrics to TensorBoard.
avg_ckpts: Whether to compute average external evaluation scores.
dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
internal development evaluation.
test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
internal testing evaluation.
dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
external development evaluation.
test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
Can be used to pass in additional inputs necessary for running the
external testing evaluation.
Returns:
Triple containing results summary, global step Tensorflow Variable and
metrics in this order.
"""

Line number: 320
"""Wrapper for running sample_decode, internal_eval and external_eval.

Args:
model_dir: Directory from which to load models from.
infer_model: Inference model for which to compute perplexities.
infer_sess: Inference TensorFlow session.
eval_model: Evaluation model for which to compute perplexities.
eval_sess: Evaluation TensorFlow session.
hparams: Model hyper-parameters.
summary_writer: Summary writer for logging metrics to TensorBoard.
sample_src_data: sample of source data for sample decoding.
sample_tgt_data: sample of target data for sample decoding.
avg_ckpts: Whether to compute average external evaluation scores.
Returns:
Triple containing results summary, global step Tensorflow Variable and
metrics in this order.
"""

Line number: 345
"""Initialize statistics that we want to accumulate."""

Line number: 347
"predict_count": 0.0,  # word count on the target side

Line number: 348
"word_count": 0.0,  # word counts for both source and target

Line number: 349
"sequence_count": 0.0,  # number of training examples processed

Line number: 354
"""Update stats: write summary and accumulate statistics."""

Line number: 357
# Update statistics

Line number: 371
"""Print all info at the current global step."""

Line number: 381
"""Add stuffs in info to summaries."""

Line number: 389
"""Update info and check for overflow."""

Line number: 390
# Per-step info

Line number: 396
# Per-predict info

Line number: 400
# Check for overflow

Line number: 413
"""Misc tasks to do before training."""

Line number: 425
# Initialize all of the iterators

Line number: 436
"""Get the right model class depending on configuration."""

Line number: 451
"""Train a translation model."""

Line number: 463
# Create model

Line number: 469
# Preload data for sample decoding.

Line number: 478
# Log and output files

Line number: 483
# TensorFlow model

Line number: 499
# Summary writer

Line number: 503
# First evaluation

Line number: 514
# This is the training loop.

Line number: 518
### Run a step ###

Line number: 524
# Finished going through the training dataset.  Go to next epoch.

Line number: 543
# Process step_result, accumulate stats, and write summary

Line number: 548
# Once in a while, we print statistics.

Line number: 558
# Reset statistics

Line number: 566
# Save checkpoint

Line number: 572
# Evaluate on dev/test

Line number: 582
# Save checkpoint

Line number: 598
# Done training

Line number: 640
"""Format results."""

Line number: 654
"""Summary of the current best results."""

Line number: 663
"""Computing perplexity."""

Line number: 673
"""Pick a sentence and decode."""

Line number: 686
# get the top translation.

Line number: 698
# Summary

Line number: 706
"""External evaluation such as BLEU and ROUGE scores."""

Line number: 731
# Save on best metrics

Line number: 741
# metric: larger is better


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/common_test_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Common utility functions for tests."""

Line number: 40
"""Create training and inference test hparams."""

Line number: 43
# TODO(rzhao): Put num_residual_layers computation logic into

Line number: 44
# `model_utils.py`, so we can also test it here.

Line number: 49
# Networks

Line number: 59
# Attention mechanisms

Line number: 63
# Train

Line number: 68
# Infer

Line number: 73
# Misc

Line number: 78
# Vocab

Line number: 88
# For inference.py test

Line number: 99
"""Create test iterator."""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/evaluation_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Utility for evaluating various tasks, e.g., translation & summarization."""

Line number: 32
"""Pick a metric and evaluate depending on task."""

Line number: 33
# BLEU scores for translation task

Line number: 37
# ROUGE scores for summarization tasks

Line number: 52
"""Clean and handle BPE or SPM outputs."""

Line number: 55
# BPE

Line number: 59
# SPM

Line number: 66
# Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py

Line number: 68
"""Compute BLEU scores and handling BPE."""

Line number: 93
# bleu_score, precisions, bp, ratio, translation_length, reference_length

Line number: 100
"""Compute ROUGE scores and handling BPE."""

Line number: 118
"""Compute accuracy, each line contains a label."""

Line number: 134
"""Compute accuracy on per word basis."""

Line number: 154
"""Compute BLEU scores using Moses multi-bleu.perl script."""

Line number: 156
# TODO(thangluong): perform rewrite using python

Line number: 157
# BPE

Line number: 161
# TODO(thangluong): not use shell=True, can be a security hazard

Line number: 176
# subprocess

Line number: 177
# TODO(thangluong): not use shell=True, can be a security hazard

Line number: 180
# extract BLEU score


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/evaluation_utils_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Tests for evaluation_utils.py."""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/__init__.py


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/iterator_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 15
"""For loading data into NMT models."""

Line number: 28
# NOTE(ebrevdo): When we subclass this, instances' __dict__ becomes empty.

Line number: 53
# Convert the word strings to character ids

Line number: 57
# Convert the word strings to ids

Line number: 61
# Add in the word counts.

Line number: 73
# The entry is the source line rows;

Line number: 74
# this has unknown-length vectors.  The last entry is

Line number: 75
# the source row size; this is a scalar.

Line number: 77
tf.TensorShape([None]),  # src

Line number: 78
tf.TensorShape([])),  # src_len

Line number: 79
# Pad the source sequences with eos tokens.

Line number: 80
# (Though notice we don't generally need to do this since

Line number: 81
# later on we will be masking out calculations past the true sequence.

Line number: 83
src_eos_id,  # src

Line number: 84
0))  # src_len -- unused

Line number: 141
# Filter zero length input sequences.

Line number: 154
# Convert the word strings to ids.  Word strings that are not in the

Line number: 155
# vocab get the lookup table's default_value integer.

Line number: 168
# Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.

Line number: 174
# Add in sequence lengths.

Line number: 190
# Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)

Line number: 194
# The first three entries are the source and target line rows;

Line number: 195
# these have unknown-length vectors.  The last two entries are

Line number: 196
# the source and target row sizes; these are scalars.

Line number: 198
tf.TensorShape([None]),  # src

Line number: 199
tf.TensorShape([None]),  # tgt_input

Line number: 200
tf.TensorShape([None]),  # tgt_output

Line number: 201
tf.TensorShape([]),  # src_len

Line number: 202
tf.TensorShape([])),  # tgt_len

Line number: 203
# Pad the source and target sequences with eos tokens.

Line number: 204
# (Though notice we don't generally need to do this since

Line number: 205
# later on we will be masking out calculations past the true sequence.

Line number: 207
src_eos_id,  # src

Line number: 208
tgt_eos_id,  # tgt_input

Line number: 209
tgt_eos_id,  # tgt_output

Line number: 210
0,  # src_len -- unused

Line number: 211
0))  # tgt_len -- unused

Line number: 216
# Calculate bucket_width by maximum source sequence length.

Line number: 217
# Pairs with length [0, bucket_width) go to bucket 0, length

Line number: 218
# [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length

Line number: 219
# over ((num_bucket-1) * bucket_width) words all go into the last bucket.

Line number: 225
# Bucket sentence pairs by the length of their source sentence and target

Line number: 226
# sentence.


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/iterator_utils_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Tests for iterator_utils.py"""

Line number: 77
[[2, 0, 3],   # c a eos -- eos is padding

Line number: 78
[-1, -1, 0]], # "f" == unknown, "e" == unknown, a

Line number: 82
[[4, 1, 2],   # sos b c

Line number: 83
[4, 2, 2]],  # sos c c

Line number: 86
[[1, 2, 3],   # b c eos

Line number: 87
[2, 2, 3]],  # c c eos

Line number: 95
[[2, 2, 0]],  # c c a

Line number: 99
[[4, 0, 1]],  # sos a b

Line number: 102
[[0, 1, 3]],  # a b eos

Line number: 157
[[2, 0, 3],     # c a eos -- eos is padding

Line number: 158
[-1, -1, 0]],  # "f" == unknown, "e" == unknown, a

Line number: 162
[[4, 1, 2],   # sos b c

Line number: 163
[4, 2, 2]],  # sos c c

Line number: 166
[[1, 2, 3],   # b c eos

Line number: 167
[2, 2, 3]],  # c c eos

Line number: 222
[[-1, -1, 0]], # "f" == unknown, "e" == unknown, a

Line number: 226
[[4, 2, 2]],   # sos c c

Line number: 229
[[2, 2, 3]],   # c c eos

Line number: 236
# Re-init iterator with skip_count=0.

Line number: 243
[[-1, -1, 0],  # "f" == unknown, "e" == unknown, a

Line number: 244
[2, 0, 3]],   # c a eos -- eos is padding

Line number: 248
[[4, 2, 2],   # sos c c

Line number: 249
[4, 1, 2]],  # sos b c

Line number: 252
[[2, 2, 3],   # c c eos

Line number: 253
[1, 2, 3]],  # b c eos

Line number: 261
[[2, 2, 0]],  # c c a

Line number: 265
[[4, 0, 1]],  # sos a b

Line number: 268
[[0, 1, 3]],  # a b eos

Line number: 304
[[2, 2, 0],   # c c a

Line number: 305
[2, 0, 3]],  # c a eos

Line number: 311
[[-1, 3, 3],    # "d" == unknown, eos eos

Line number: 312
[-1, -1, 0]],  # "f" == unknown, "e" == unknown, a


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/misc_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Generally useful utility functions."""

Line number: 34
# LINT.IfChange

Line number: 36
# LINT.ThenChange(<pwd>/nmt/copy.bara.sky)

Line number: 43
"""Exponentiation with catching of overflow error."""

Line number: 52
"""Take a start time, print elapsed duration, and return a new time."""

Line number: 59
"""Similar to print but with support to flush and output to a file."""

Line number: 68
# stdout

Line number: 80
"""Print hparams, can skip keys based on pattern."""

Line number: 90
"""Load hparams from an existing model directory."""

Line number: 107
"""Override hparams values with existing standard hparams config."""

Line number: 116
"""Save hparams."""

Line number: 124
"""Print the shape and value of a tensor at test time. Return a new tensor."""

Line number: 131
"""Add a new summary to the current summary_writer.
Useful to log things that are not part of the training graph, e.g., tag=BLEU.
"""

Line number: 140
# GPU options:

Line number: 141
# https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html

Line number: 147
# CPU threads options

Line number: 157
"""Convert a sequence words into sentence."""

Line number: 158
if (not hasattr(words, "__len__") and  # for numpy array

Line number: 165
"""Convert a sequence of bpe words into sentence."""

Line number: 174
else:  # end of a word

Line number: 182
"""Decode a text in SPM (https://github.com/google/sentencepiece) format."""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/misc_utils_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Tests for vocab_utils."""


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/nmt_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Utility functions specifically for NMT."""

Line number: 42
"""Decode a test set and compute a score according to the evaluation task."""

Line number: 43
# Decode

Line number: 51
trans_f.write("")  # Write empty string to ensure file is created.

Line number: 81
# Evaluation

Line number: 97
"""Given batch decoding outputs, select a sentence and turn to text."""

Line number: 99
# Select a sentence

Line number: 102
# If there is an eos symbol in outputs, cut them at that point.

Line number: 106
if subword_option == "bpe":  # BPE

Line number: 108
elif subword_option == "spm":  # SPM


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/standard_hparams_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""standard hparams utils."""

Line number: 27
# Data

Line number: 37
# Networks

Line number: 50
# Attention mechanisms

Line number: 56
# Train

Line number: 70
# Data constraints

Line number: 78
# Data format

Line number: 85
# Misc

Line number: 88
epoch_step=0,  # record where we were within an epoch.

Line number: 95
# only enable beam search during inference when beam_width > 0.

Line number: 103
# For inference

Line number: 110
# Language model


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/vocab_utils.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Utility to handle vocabularies."""

Line number: 30
# word level special token

Line number: 36
# char ids 0-255 come from utf-8 encoding bytes

Line number: 37
# assign 256-300 to special chars

Line number: 38
BOS_CHAR_ID = 256  # <begin sentence>

Line number: 39
EOS_CHAR_ID = 257  # <end sentence>

Line number: 40
BOW_CHAR_ID = 258  # <begin word>

Line number: 41
EOW_CHAR_ID = 259  # <end word>

Line number: 42
PAD_CHAR_ID = 260  # <padding>

Line number: 44
DEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.

Line number: 48
"""Given string and length, convert to byte seq of at most max_length.

This process mimics docqa/elmo's preprocessing:
https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py

Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py &
our usage differs from docqa/elmo.

Args:
text: tf.string tensor of shape []
max_length: max number of chars for each word.

Returns:
A tf.int32 tensor of the byte encoded text.
"""

Line number: 76
"""Given a sequence of strings, map to sequence of bytes.

Args:
tokens: A tf.string tensor

Returns:
A tensor of shape words.shape + [bytes_per_word] containing byte versions
of each word.
"""

Line number: 113
"""Check if vocab_file doesn't exist, create from corpus_file."""

Line number: 118
# Verify if the vocab starts with unk, sos, eos

Line number: 119
# If not, prepend those tokens & generate a new vocab file

Line number: 144
"""Creates vocab tables for src_vocab_file and tgt_vocab_file."""

Line number: 156
"""Load embed_file into a python dictionary.

Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming
Here is an exampe assuming embed_size=5:

the -0.071549 0.093459 0.023738 -0.090339 0.056123
to 0.57346 0.5417 -0.23477 -0.3624 0.4037
and 0.20327 0.47348 0.050877 0.002103 0.060547

For word2vec format, the first line will be: <num_words> <emb_size>.

Args:
embed_file: file path to the embedding file.
Returns:
a dictionary that maps word to vector, and the size of embedding dimensions.
"""

Line number: 181
if len(tokens) == 2:  # header line


 
 ---------------------------------------------------------------------------------------------------------- 

File Name: nmt-master/nmt/utils/vocab_utils_test.py

Line number: 1
# Copyright 2017 Google Inc. All Rights Reserved.

Line number: 2
#

Line number: 3
# Licensed under the Apache License, Version 2.0 (the "License");

Line number: 4
# you may not use this file except in compliance with the License.

Line number: 5
# You may obtain a copy of the License at

Line number: 6
#

Line number: 7
#     http://www.apache.org/licenses/LICENSE-2.0

Line number: 8
#

Line number: 9
# Unless required by applicable law or agreed to in writing, software

Line number: 10
# distributed under the License is distributed on an "AS IS" BASIS,

Line number: 11
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

Line number: 12
# See the License for the specific language governing permissions and

Line number: 13
# limitations under the License.

Line number: 14
# ==============================================================================

Line number: 16
"""Tests for vocab_utils."""

Line number: 32
# Create a vocab file

Line number: 41
# Call vocab_utils

Line number: 47
# Assert: we expect the code to add  <unk>, <s>, </s> and

Line number: 48
# create a new vocab file


 
 ---------------------------------------------------------------------------------------------------------- 

